{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.   Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.   Read the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"default of credit card clients.xls\")\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the Missing Data is available or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the available missing data\n",
    "\n",
    "missing_df = df.isna().sum()\n",
    "missing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.   Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is the distribution of LIMIT_BAL (Amount of the given credit)?\n",
    "\n",
    "df['LIMIT_BAL'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the distribution of target data\n",
    "df['default payment next month'].value_counts().plot.bar() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the gender ratio in the data-set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = df['SEX'].value_counts()\n",
    "percent = count *100 /len(df)\n",
    "# Pie chart\n",
    "mylabels = ['Female','Male']\n",
    "mycolors = [\"Red\", \"Green\"]\n",
    "\n",
    "plt.pie(percent, labels=mylabels, startangle=90, shadow = True, colors=mycolors,textprops=dict(color =\"white\"),autopct='%1.1f%%')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0, 0.5, 1), title = \"Sex\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.countplot(x='SEX', data=df,hue=\"default payment next month\", palette=\"muted\")\n",
    "h,l = ax.get_legend_handles_labels()\n",
    "\n",
    "l = [\"Non-Defaulters\", \"Defaulters\"]\n",
    "\n",
    "ax.legend(h, l)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_class = df['default payment next month'].value_counts()\n",
    "percent_class = count_class *100 /len(df)\n",
    "mylabels1 = ['Non_Defaulters', 'Defaulters']\n",
    "mycolors1 = ['Green', 'Red']\n",
    "plt.pie(percent_class, labels=mylabels1, startangle=90, shadow = True, colors=mycolors1,textprops=dict(color =\"white\"),autopct='%1.1f%%')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0, 0.5, 1), title = \"Sex\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the Education Distribution ratio in the given dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edu_freq_tbl = pd.crosstab(df['EDUCATION'],'frequency').reset_index()\n",
    "edu_freq_tbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pywaffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pywaffle import Waffle\n",
    "data = {'graduate Student': 10585, 'university': 14030, 'high school': 4917,'Others':123+280+51+14}\n",
    "fig = plt.figure(\n",
    "    FigureClass=Waffle, \n",
    "    rows=5,\n",
    "    columns=20, \n",
    "    values=data,\n",
    "    figsize=(12,12),\n",
    "    legend={'loc': 'upper left', 'bbox_to_anchor': (1.1, 1)}\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.countplot(x='EDUCATION',data=df,hue=\"default payment next month\",palette=\"muted\")\n",
    "h,l = ax.get_legend_handles_labels()\n",
    "\n",
    "l = [\"Non-Defaulters\", \"Defaulters\"]\n",
    "\n",
    "ax.legend(h, l)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defaulters = df[df['default payment next month']==1]\n",
    "non_defaulters = df[df['default payment next month']==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Is there any visual impact in education distribution in both the categories  defaulters and non-defaulters?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_df_edu_tbl = pd.crosstab(defaulters['EDUCATION'],'frequency').reset_index()\n",
    "freq_df_edu_tbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'graduate Student': 2036, 'university': 3330, 'high school': 1237,'Others':7+18+8}\n",
    "fig = plt.figure(\n",
    "    FigureClass=Waffle, \n",
    "    rows=5,\n",
    "    columns=20, \n",
    "    values=data,\n",
    "    figsize=(12,12),\n",
    "    legend={'loc': 'upper left', 'bbox_to_anchor': (1.1, 1)}\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_ndf_edu_tbl = pd.crosstab(non_defaulters['EDUCATION'],'frequency').reset_index()\n",
    "freq_ndf_edu_tbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'graduate Student': 8549, 'university': 10700, 'high school': 3680,'Others':14+116+262+43}\n",
    "fig = plt.figure(\n",
    "    FigureClass=Waffle, \n",
    "    rows=5,\n",
    "    columns=20, \n",
    "    values=data,\n",
    "    figsize=(12,12),\n",
    "    legend={'loc': 'upper left', 'bbox_to_anchor': (1.1, 1)}\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax= sns.countplot(x='EDUCATION',data=df,hue=\"default payment next month\",palette=\"muted\") # where 1 reflects Yes and 0 \"No\"\n",
    "\n",
    "h,l = ax.get_legend_handles_labels()\n",
    "\n",
    "l = [\"Non-Defaulters\", \"Defaulters\"]\n",
    "\n",
    "ax.legend(h, l)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### what is the Age Distribution in complete dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['AGE'],kde=True,bins=30)  # It shows that there is a large number of clients whose age is between 25 to 40."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the Marriage distribution ratio in complete dataset (1 = married; 2=single, 3=others) ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_marriage_tbl = pd.crosstab(df['MARRIAGE'],'frequency').reset_index()\n",
    "freq_marriage_tbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Married': 13659, 'Single': 15964, 'Others': 323}\n",
    "fig = plt.figure(\n",
    "    FigureClass=Waffle, \n",
    "    rows=5,\n",
    "    columns=20, \n",
    "    values=data,\n",
    "    figsize=(12,12),\n",
    "    legend={'loc': 'upper left', 'bbox_to_anchor': (1.1, 1)}\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Is there any visual impact in marriage ratio seperatly in defaulters and non-defaulters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_df_marriage_tbl = pd.crosstab(defaulters['MARRIAGE'],'frequency').reset_index()\n",
    "freq_df_marriage_tbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Married': 3206, 'Single': 3341, 'Others': 84}\n",
    "fig = plt.figure(\n",
    "    FigureClass=Waffle, \n",
    "    rows=5,\n",
    "    columns=20, \n",
    "    values=data,\n",
    "    figsize=(12,12),\n",
    "    legend={'loc': 'upper left', 'bbox_to_anchor': (1.1, 1)}\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_ndf_marriage_tbl = pd.crosstab(non_defaulters['MARRIAGE'],'frequency').reset_index()\n",
    "freq_ndf_marriage_tbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Married': 10453, 'Single': 12623, 'Others': 239}\n",
    "fig = plt.figure(\n",
    "    FigureClass=Waffle, \n",
    "    rows=5,\n",
    "    columns=20, \n",
    "    values=data,\n",
    "    figsize=(12,12),\n",
    "    legend={'loc': 'upper left', 'bbox_to_anchor': (1.1, 1)}\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.countplot(x='MARRIAGE',data=df,hue=\"default payment next month\", palette=\"muted\")\n",
    "h,l = ax.get_legend_handles_labels()\n",
    "\n",
    "l = [\"Non-Defaulters\", \"Defaulters\"]\n",
    "\n",
    "ax.legend(h, l)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To confirm the correlation between the features in this dataset\n",
    "\n",
    "corr_matrix = df.corr()\n",
    "plt.figure(figsize=(18,18))\n",
    "sns.heatmap(corr_matrix, annot=True )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the correlation heatmap above, it can be seen that there are some relationships between the feature columns, they are not entirely independent. \n",
    "\n",
    "But in this scenario, there is a correlation because a customer was not able to pay the bill for 1 month was again not able to pay it for the subsequent months and hence the correlation is generated.\n",
    "\n",
    "Again for the bill amount column, the same has happened. If the customer was not able to pay the bill, then the bill amount almost remained the same, or if the customer was able to pay then the bill amount got reduced.\n",
    "\n",
    "We remove columns when they convey the same information. But here, dropping the columns shall result in the loss of bill and payment history data. So, we don’t need to drop any column although there is a correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the heatmap, you figured out that target variable default.payment.next.month depends on pay variables more. But I don’t suggest you drop the other features because it will be the loss of information. You can have a try of training the model with the most dependent features and evaluate the model also."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Predictive Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = df.drop(columns = ['default payment next month'], axis=1)\n",
    "y = df['default payment next month']\n",
    "\n",
    "# Lets split the training and test dataset in 80 :20 ratio\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.20,random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import  accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "log_model = LogisticRegression(random_state=1)\n",
    "log_model.fit(x_train,y_train)\n",
    "\n",
    "y_pred = log_model.predict(x_test)\n",
    "\n",
    "roc = roc_auc_score(y_test, y_pred)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred)\n",
    "rec = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "results = pd.DataFrame([['Logistic Regression', acc,prec,rec, f1,roc]],\n",
    "columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score','ROC'])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, cmap=\"Blues\", annot=True,annot_kws={\"size\": 16})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_estimators = 100,criterion = 'entropy',random_state = 0)\n",
    "rfc.fit(x_train,y_train)\n",
    "y_pred = rfc.predict(x_test)\n",
    "roc=roc_auc_score(y_test, y_pred)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred)\n",
    "rec = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "results = pd.DataFrame([['Random tree Classifier', acc, prec, rec, f1, roc]],\n",
    "columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score','ROC'])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, cmap=\"Blues\", annot=True,annot_kws={\"size\": 16})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  XGBOOST Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply XGBoost classifier model\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(x_train, y_train)\n",
    "y_pred =xgb.predict(x_test)\n",
    "roc=roc_auc_score(y_test, y_pred)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred)\n",
    "rec = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "results = pd.DataFrame([['XGBOOST Classifier', acc,prec,rec, f1,roc]],\n",
    "columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score','ROC'])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, cmap=\"Blues\", annot=True,annot_kws={\"size\": 16})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
